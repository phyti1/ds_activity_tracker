{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Model\n",
    "\n",
    "This notebook is used to train a combined model with the clustering and the Random forest Model. The purpose of this model is to have a single Object which can then be nicely integrated to the webservice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tslearn.clustering import TimeSeriesKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Since Ronny and I use the same preprocessing, I used the preprocessed data, which he exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('../train.npz')\n",
    "test = np.load('../test.npz')\n",
    "\n",
    "X_train = train['X']\n",
    "y_train = train['y']\n",
    "X_test = test['X']\n",
    "y_test = test['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33887, 43, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we work with 33887 samples to train our model on. Each sample is a matrix of size 43x13, where the 13 indicates the number of features we have per sample and the 43 are the number of measurement points in each sample (43 points ~ 2.5 Seconds).\n",
    "\n",
    "The 13 features are divied into 4 sensors, with every sensor measuring 3 Axis except the last one, which is records a fourth axis.\n",
    "The sensors used Are:\n",
    "Accerometer, Magnetometer, Gyroscope, and an Orientation Sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "I decided to go with a bit more of a special Model, which first fits a Dynamic Time Warping Clustering model on each feature, and then uses the resulting distances to the centroids as input for a random Forest Classifier. I also expanded the capabilities a bit, so that one can add commen statistical parameters like var, min, max... to the data before it goes into the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringForest():\n",
    "    def __init__(self, combineSensors=True, useEuclidCombineDistance=True, useStatisialParameters=False, useAllActivities=True, seed=420):\n",
    "        self.combineOnSensor = combineSensors\n",
    "        self.useEuclidCombineDistance = useEuclidCombineDistance\n",
    "        self.useStatisialParameters = useStatisialParameters\n",
    "        self.useAllActivities = useAllActivities\n",
    "        self.seed = seed\n",
    "        self.numberOfActivities = 7 if useAllActivities else 4\n",
    "        self.numberOfSensors = 4 if combineSensors else 13\n",
    "        self.models = None\n",
    "        self.best_estimator = None\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, maxIter=5, n_jobs=-1):\n",
    "        # preprocess the data as needed\n",
    "        preprocessedX = self.preprocess_X(X)\n",
    "\n",
    "        self.fit_time_series_clustering(preprocessedX, maxIter=maxIter, n_jobs=n_jobs)\n",
    "        distances = self.get_distances(preprocessedX)\n",
    "\n",
    "        self.fit_random_forest(distances, y, n_jobs=n_jobs)\n",
    "        return self\n",
    "\n",
    "    def fit_time_series_clustering(self, preprocessedX, maxIter=5, n_jobs=-1):\n",
    "        self.models = []\n",
    "        for i in range(4):\n",
    "            print(\"fitting model number {}\".format(i))\n",
    "            model = TimeSeriesKMeans(n_clusters=self.numberOfActivities, metric=\"dtw\",\n",
    "                                    max_iter=maxIter, random_state=self.seed, verbose=False, n_jobs=n_jobs)  # use number of activities as number of clusters\n",
    "            model.fit(preprocessedX[:, :, i])\n",
    "            self.models.append(model)\n",
    "\n",
    "    def plot_centroids(self, names, size = (20, 20)):\n",
    "        fig, ax = plt.subplots(self.numberOfActivities, self.numberOfSensors, figsize=size)\n",
    "        # model names\n",
    "        # plot the centroids for each model\n",
    "        for i in range(self.numberOfSensors):\n",
    "            for j in range(self.numberOfActivities):\n",
    "                ax[j, i].plot(self.models[i].cluster_centers_[j])\n",
    "                \n",
    "            # set the tiltle per column of subplot\n",
    "            ax[0, i].set_title(names[i])\n",
    "\n",
    "    def fit_random_forest(self, distances, y_train, n_jobs=-1):\n",
    "        random_forest = RandomForestClassifier(random_state=self.seed)\n",
    "        # create random forest grid search\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [15, 30, 45],\n",
    "            'min_samples_split': [2, 6],\n",
    "        }\n",
    "\n",
    "        # run gridSerach\n",
    "        grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, n_jobs=-n_jobs, verbose=5)\n",
    "        grid_search.fit(distances, y_train)\n",
    "        self.best_estimator = grid_search.best_estimator_\n",
    "\n",
    "    def preprocess_X(self, X):\n",
    "        if (self.combineOnSensor):\n",
    "            if (self.useEuclidCombineDistance):\n",
    "                pre_pros = self.combineAxisOnSensorsEuclidLength(X)\n",
    "            else:\n",
    "                pre_pros = self.combineAxisOnSensors(X)\n",
    "        else:\n",
    "            pre_pros = X\n",
    "        return pre_pros\n",
    "\n",
    "    def predict(self, X):\n",
    "        if (self.best_estimator is None):\n",
    "            return None\n",
    "        \n",
    "        prepros = self.preprocess_X(X)\n",
    "\n",
    "        # stack distances horizontally\n",
    "        distances = self.get_distances(prepros)\n",
    "\n",
    "        return self.best_estimator.predict(distances)\n",
    "\n",
    "    def get_distances(self, X):\n",
    "        distances = []\n",
    "        for i in range(len(self.models)):\n",
    "            distances.append(self.models[i].transform(X[:, :, i]))\n",
    "\n",
    "        distances = np.hstack(distances)\n",
    "        if (self.useStatisialParameters):\n",
    "            # stack statistical parameters to distances\n",
    "            distances = np.hstack((distances, ClusteringForest.calclateStatical_measurements(X)))\n",
    "        return distances\n",
    "\n",
    "    def score(self, X, y):\n",
    "        if (self.best_estimator is None):\n",
    "            return None\n",
    "        else:\n",
    "            prepros = self.preprocess_X(X)\n",
    "            distances = self.get_distances(prepros)\n",
    "            return self.best_estimator.score(distances, y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calclateStatical_measurements(X):\n",
    "        # calculate mean, min, max and std for each sensor time series\n",
    "        statsMeasurements = []\n",
    "        for j in range(X.shape[0]): # for each sample\n",
    "            statsMeasurements.append([]) # add empty list for each sample\n",
    "            for i in range(X.shape[-1]): # for each sensor\n",
    "                statsMeasurements[j].append(np.mean(X[j, :, i]))\n",
    "                statsMeasurements[j].append(np.min(X[j, :, i]))\n",
    "                statsMeasurements[j].append(np.max(X[j, :, i]))\n",
    "                statsMeasurements[j].append(np.std(X[j, :, i]))\n",
    "        return statsMeasurements\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def combineAxisOnSensors(input):\n",
    "        s, n, f = input.shape\n",
    "        output = np.zeros((s, n, 4))\n",
    "        # for each sample, sum rows 0-2\n",
    "        output[:, :, 0] = input[:, :, 0] + input[:, :, 1] + input[:, :, 2]\n",
    "        # for each sample, sum rows 3-5\n",
    "        output[:, :, 1] = input[:, :, 3] + input[:, :, 4] + input[:, :, 5]\n",
    "        # for each sample, sum rows 6-8\n",
    "        output[:, :, 2] = input[:, :, 6] + input[:, :, 7] + input[:, :, 8]\n",
    "        # for each sample, sum rows 9-12\n",
    "        output[:, :, 3] = input[:, :, 9] + input[:, :, 10] + input[:, :, 11] + input[:, :, 12]\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def combineAxisOnSensorsEuclidLength(input):\n",
    "        s, n, _ = input.shape\n",
    "        output = np.zeros((s, n, 4))\n",
    "        # for each sample, sum rows 0-2\n",
    "        output[:, :, 0] = np.sqrt(input[:, :, 0] ** 2 + input[:, :, 1] ** 2 + input[:, :, 2] ** 2)\n",
    "        # for each sample, sum rows 3-5\n",
    "        output[:, :, 1] = np.sqrt(input[:, :, 3] ** 2 + input[:, :, 4] ** 2 + input[:, :, 5] ** 2)\n",
    "        # for each sample, sum rows 6-8\n",
    "        output[:, :, 2] = np.sqrt(input[:, :, 6] ** 2 + input[:, :, 7] ** 2 + input[:, :, 8] ** 2)\n",
    "        # for each sample, sum rows 9-12\n",
    "        output[:, :, 3] = np.sqrt(input[:, :, 9] ** 2 + input[:, :, 10] ** 2 + input[:, :, 11] ** 2 + input[:, :, 12] ** 2)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Model\n",
    "\n",
    "Now we want to fit the model, for the we use our wrapper class and train it right there. To test the implementation we just use a very small portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model number 0\n",
      "fitting model number 1\n",
      "fitting model number 2\n",
      "fitting model number 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 677 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 677 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 677 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 677 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=   0.9s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.6s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=   0.8s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=   0.8s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=   0.8s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=   0.9s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=   0.8s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=   0.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ClusteringForest at 0x7fd6c12cc8e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select 20% of samples to train on\n",
    "def getTrainingSamples(X, y, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    training_idx = indices[:int(X.shape[0] * 0.02)]\n",
    "    return X[training_idx], y[training_idx]\n",
    "\n",
    "small_X, small_y = getTrainingSamples(X_train, y_train, seed=0)\n",
    "\n",
    "basemodel = ClusteringForest(useStatisialParameters=True)\n",
    "basemodel.fit(small_X, small_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7707743153918791"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 3, 5, ..., 6, 5, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see the model works, let it run with all samples and different parameters and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# safe model to file\n",
    "with open('basemodel.pkl', 'wb') as f:\n",
    "    pickle.dump(basemodel, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the real Models\n",
    "\n",
    "Now that we know our model works, we are ready to train it on the whole data. I will just train a bunch of models with different parameters and then choose the one with the best score. Normally we would need to use an extra validation set to choose the best model on becasue our decision now could be biased towards the test set. However since our test set is very large and the diffrences in accuracy are not all to small, the probability of choosing the wrong model becuase it overfits the test set vanishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model number 0\n",
      "fitting model number 1\n",
      "fitting model number 2\n",
      "fitting model number 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   9.1s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   9.2s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   9.1s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   9.1s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=   9.2s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  27.5s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  27.2s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  27.1s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  27.1s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  27.1s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  45.1s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  45.2s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  45.1s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  45.1s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  45.0s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   9.0s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   9.0s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   8.9s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   9.0s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=   9.0s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  26.8s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  26.9s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  26.8s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  26.9s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  26.7s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  44.6s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  44.9s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  44.7s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  44.7s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  44.6s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  10.1s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  29.9s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  30.2s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  29.9s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  30.0s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  30.0s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  50.2s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  50.4s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  50.1s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  50.2s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  50.2s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  10.0s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  10.0s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   9.9s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  10.0s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  10.0s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  29.9s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  30.0s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  29.7s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  29.8s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  29.9s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=  49.4s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=  49.9s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=  49.5s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=  49.6s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=  49.6s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  10.1s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  10.1s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  30.1s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  30.3s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  30.4s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  30.5s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  30.1s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=  50.2s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=  50.3s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=  50.0s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=  50.0s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time=  50.2s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  10.0s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=   9.9s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  10.0s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=   9.9s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  10.0s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  29.9s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  30.0s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  29.8s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  29.7s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  29.9s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=  49.6s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=  49.8s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=  49.6s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=  49.7s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time=  49.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846081208687441\n"
     ]
    }
   ],
   "source": [
    "euclid_no_stats_model = ClusteringForest(useStatisialParameters=False, useEuclidCombineDistance=True)\n",
    "euclid_no_stats_model.fit(X_train, y_train)\n",
    "print(euclid_no_stats_model.score(X_test, y_test))\n",
    "\n",
    "# save file as pickle\n",
    "with open('euclid_no_stats_model.pkl', 'wb') as f:\n",
    "    pickle.dump(euclid_no_stats_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model number 0\n",
      "fitting model number 1\n",
      "fitting model number 2\n",
      "fitting model number 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  10.6s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  10.7s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  10.6s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  10.6s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  10.6s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  31.9s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  32.1s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  32.1s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  32.2s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  31.9s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  53.2s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  53.3s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  53.2s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  53.2s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  53.2s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  10.6s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  10.6s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  10.5s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  10.6s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  10.6s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  31.7s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  31.9s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  31.9s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  31.7s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  31.8s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  53.0s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  53.0s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  52.9s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  52.9s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  53.1s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  12.0s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  12.0s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  36.0s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  36.3s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  35.5s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  35.7s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  35.5s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  59.2s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  59.6s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  59.3s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  59.5s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time=  59.2s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  11.9s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  12.2s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  11.8s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  11.8s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  11.8s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  35.3s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  35.7s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  35.2s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  36.8s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  35.4s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time=10.9min\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 6.0min\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 6.0min\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 6.0min\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 7.9min\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  12.0s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time= 5.2min\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  53.3s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  35.9s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time= 5.6min\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time= 5.2min\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=19.0min\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.0min\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.0min\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.0min\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.0min\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.0min\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  12.2s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  12.1s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  12.0s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  12.0s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  12.1s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  36.8s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  36.6s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  36.1s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  36.2s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  36.2s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.0min\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.0min\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.0min\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.0min\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8884560906515581\n"
     ]
    }
   ],
   "source": [
    "euclid_with_stats_model = ClusteringForest(useStatisialParameters=True, useEuclidCombineDistance=True)\n",
    "euclid_with_stats_model.fit(X_train, y_train)\n",
    "print(euclid_with_stats_model.score(X_test, y_test))\n",
    "\n",
    "# save file as pickle\n",
    "with open('euclid_with_stats_model.pkl', 'wb') as f:\n",
    "    pickle.dump(euclid_with_stats_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model number 0\n",
      "fitting model number 1\n",
      "fitting model number 2\n",
      "fitting model number 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 33887 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  11.8s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  11.6s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  11.6s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  11.5s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=100; total time=  11.6s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  34.8s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  34.8s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  34.7s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  34.7s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=300; total time=  34.7s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  57.8s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  57.5s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  57.4s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  57.1s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=2, n_estimators=500; total time=  57.2s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  11.5s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  11.5s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  11.6s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  11.6s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=100; total time=  11.4s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  34.2s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  34.0s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  35.4s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  34.4s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=300; total time=  34.0s\n",
      "[CV 1/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  56.8s\n",
      "[CV 2/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  56.7s\n",
      "[CV 3/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  56.5s\n",
      "[CV 4/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  56.9s\n",
      "[CV 5/5] END max_depth=15, min_samples_split=6, n_estimators=500; total time=  56.8s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  13.0s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  13.0s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  13.3s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  13.2s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=100; total time=  13.0s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  39.7s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  40.5s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  39.9s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  39.7s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  40.2s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  13.2s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  13.5s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  13.1s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  13.5s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=100; total time=  13.6s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  40.0s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  39.0s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  38.7s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  38.7s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  38.6s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 2/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 3/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 4/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 5/5] END max_depth=30, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  13.1s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  12.9s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  13.0s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  13.1s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=100; total time=  12.9s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  39.2s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  39.1s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  38.9s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  39.3s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=300; total time=  39.1s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 2/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 3/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 4/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 5/5] END max_depth=45, min_samples_split=2, n_estimators=500; total time= 1.1min\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  13.2s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  13.1s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  13.1s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  13.4s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=100; total time=  13.0s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  39.5s\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  39.3s\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  39.1s\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  39.3s\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=300; total time=  39.1s\n",
      "[CV 1/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 2/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 3/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 4/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.1min\n",
      "[CV 5/5] END max_depth=45, min_samples_split=6, n_estimators=500; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "/Users/flavio/opt/anaconda3/lib/python3.8/site-packages/tslearn/utils/utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 8472 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9147780925401322\n"
     ]
    }
   ],
   "source": [
    "normal_with_stats_model = ClusteringForest(useStatisialParameters=True, useEuclidCombineDistance=False)\n",
    "normal_with_stats_model.fit(X_train, y_train)\n",
    "print(normal_with_stats_model.score(X_test, y_test))\n",
    "\n",
    "# save file as pickle\n",
    "with open('normal_with_stats_model.pkl', 'wb') as f:\n",
    "    pickle.dump(normal_with_stats_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we found the best model. This will now be integrated in the webservice via pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3daccf1105b006a6ea4501cd36e3402ce8b498148648c6f2c8d08021d314ddd1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
